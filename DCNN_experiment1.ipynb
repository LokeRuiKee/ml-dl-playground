{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNeVkMkkQXg3g6OKGEga5x3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeRuiKee/ml-dl-playground/blob/main/DCNN_experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Convolutional Network Template Code"
      ],
      "metadata": {
        "id": "6aij1LhWGdgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                                               # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='relu'),                                             # Convolutional layer with 128 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                                           # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n"
      ],
      "metadata": {
        "id": "HCGOpaC9GhQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz1eN47OGhoS",
        "outputId": "c63670d0-1514-4b43-f17c-b8605aaa879a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 58s 30ms/step - loss: 0.2669 - accuracy: 0.9180\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0917 - accuracy: 0.9739\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0661 - accuracy: 0.9806\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0522 - accuracy: 0.9851\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 57s 31ms/step - loss: 0.0417 - accuracy: 0.9882\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0488 - accuracy: 0.9872\n",
            "Test accuracy: 0.9872000217437744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 activation, optimizer = adam"
      ],
      "metadata": {
        "id": "ymf36_viB_K7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## relu"
      ],
      "metadata": {
        "id": "_4yu35XnCTFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                                               # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='relu'),                                             # Convolutional layer with 128 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                                           # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "gUtaPMAQCD06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60f0fdff-5229-455c-d20a-8aeda43ec406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 58s 30ms/step - loss: 0.2577 - accuracy: 0.9199\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0837 - accuracy: 0.9761\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0612 - accuracy: 0.9820\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0480 - accuracy: 0.9857\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0394 - accuracy: 0.9883\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0474 - accuracy: 0.9865\n",
            "Test accuracy: 0.9865000247955322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeakyReLU"
      ],
      "metadata": {
        "id": "60e53jC6CVdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "XD_jeVVDCXDw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "455a4d6b-06a3-4582-fa54-9fe845643981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 61s 31ms/step - loss: 0.2070 - accuracy: 0.9362\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0717 - accuracy: 0.9786\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 0.0542 - accuracy: 0.9833\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0413 - accuracy: 0.9871\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0357 - accuracy: 0.9890\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0414 - accuracy: 0.9880\n",
            "Test accuracy: 0.9879999756813049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## elu"
      ],
      "metadata": {
        "id": "n_zHPQ0uCXey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='elu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and elu activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='elu'),                                               # Convolutional layer with 64 filters of size 3x3 and elu activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='elu'),                                             # Convolutional layer with 128 filters of size 3x3 and elu activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='elu'),                                                           # Fully connected layer with 128 neurons and elu activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "jsARw1b5Cccd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1f80006-32e6-4852-a58c-387d4a5793ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.2103 - accuracy: 0.9353\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0805 - accuracy: 0.9754\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 63s 33ms/step - loss: 0.0651 - accuracy: 0.9805\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 63s 34ms/step - loss: 0.0537 - accuracy: 0.9837\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0449 - accuracy: 0.9862\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0565 - accuracy: 0.9847\n",
            "Test accuracy: 0.9847000241279602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PReLU"
      ],
      "metadata": {
        "id": "QpYS5REYCapR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='PReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and PReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='PReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and PReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='PReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and PReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='PReLU'),                                                           # Fully connected layer with 128 neurons and PReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "ixWdbUp_CZTb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa72b642-3ef6-4ea5-bf11-4904a1447b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.2487 - accuracy: 0.9228\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 72s 38ms/step - loss: 0.0775 - accuracy: 0.9770\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0557 - accuracy: 0.9834\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0432 - accuracy: 0.9874\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 72s 39ms/step - loss: 0.0375 - accuracy: 0.9890\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0670 - accuracy: 0.9825\n",
            "Test accuracy: 0.9825000166893005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid"
      ],
      "metadata": {
        "id": "f4qrXo7RCdXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='sigmoid', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and PReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='sigmoid'),                                               # Convolutional layer with 64 filters of size 3x3 and sigmoid activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='sigmoid'),                                             # Convolutional layer with 128 filters of size 3x3 and sigmoid activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='sigmoid'),                                                           # Fully connected layer with 128 neurons and sigmoid activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "wP9aE1VzCe-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e107ca58-04d4-4f6f-e55d-b49251cbd200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 63s 33ms/step - loss: 2.3112 - accuracy: 0.1069\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 1.6855 - accuracy: 0.3717\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 0.2454 - accuracy: 0.9286\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.1545 - accuracy: 0.9549\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.1194 - accuracy: 0.9653\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.1006 - accuracy: 0.9687\n",
            "Test accuracy: 0.9686999917030334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tanh"
      ],
      "metadata": {
        "id": "RHxqTvZ5CfS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and tanh activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='tanh'),                                               # Convolutional layer with 64 filters of size 3x3 and tanh activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='tanh'),                                             # Convolutional layer with 128 filters of size 3x3 and tanh activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='tanh'),                                                           # Fully connected layer with 128 neurons and tanh activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "ixp7yvp5Cf_-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b60928-1103-4eba-8f41-7c0cabd92aa9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 63s 33ms/step - loss: 0.2022 - accuracy: 0.9383\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0808 - accuracy: 0.9756\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.0653 - accuracy: 0.9801\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0511 - accuracy: 0.9844\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0441 - accuracy: 0.9867\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0548 - accuracy: 0.9828\n",
            "Test accuracy: 0.9828000068664551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## softmax"
      ],
      "metadata": {
        "id": "GmvYNXfPCgaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='softmax', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and softmax activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='softmax'),                                               # Convolutional layer with 64 filters of size 3x3 and softmax activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='softmax'),                                             # Convolutional layer with 128 filters of size 3x3 and softmax activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='softmax'),                                                           # Fully connected layer with 128 neurons and softmax activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "jzWjTZ9lChyy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af9d761-6739-452a-f220-ece8a7cb3fd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 70s 37ms/step - loss: 2.3015 - accuracy: 0.1124\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 2.3015 - accuracy: 0.1120\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 72s 39ms/step - loss: 2.3016 - accuracy: 0.1123\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 2.2125 - accuracy: 0.1749\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 1.5910 - accuracy: 0.4165\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 1.2941 - accuracy: 0.5949\n",
            "Test accuracy: 0.5949000120162964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conclusion\n",
        "Best activation function: LeakyReLU. Test accuracy: 0.9879999756813049"
      ],
      "metadata": {
        "id": "-oBOA2DcCiVV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best activation function: LeakyReLU, 8 optimizer"
      ],
      "metadata": {
        "id": "Wu8YJ-E8Cmeg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD"
      ],
      "metadata": {
        "id": "zVqyiTwaCpJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "WpaHgRDBCm2n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f97337e-3ed4-4aab-ab5f-ccccc8521e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.9880 - accuracy: 0.6831\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.2441 - accuracy: 0.9251\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.1730 - accuracy: 0.9475\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.1377 - accuracy: 0.9581\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.1200 - accuracy: 0.9642\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0752 - accuracy: 0.9764\n",
            "Test accuracy: 0.9764000177383423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD with Momentum"
      ],
      "metadata": {
        "id": "PeVBevxhCqV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "SGDm = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer=SGDm,\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "QwWszRVRCrLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a90b9ae3-6f14-4394-ff26-d9d979fe1bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 60s 31ms/step - loss: 0.2994 - accuracy: 0.9032\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0861 - accuracy: 0.9733\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0618 - accuracy: 0.9810\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0499 - accuracy: 0.9846\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0408 - accuracy: 0.9878\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0496 - accuracy: 0.9868\n",
            "Test accuracy: 0.9868000149726868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp"
      ],
      "metadata": {
        "id": "NkuTPgzcCrso"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='RMSProp',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "loaZPbt1Csxn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd78bd92-6ac5-4b01-fe30-dfee4818d20e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 60s 31ms/step - loss: 0.2058 - accuracy: 0.9373\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0735 - accuracy: 0.9783\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0579 - accuracy: 0.9830\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0485 - accuracy: 0.9862\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0424 - accuracy: 0.9877\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0571 - accuracy: 0.9843\n",
            "Test accuracy: 0.9843000173568726\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adam"
      ],
      "metadata": {
        "id": "hdg-1l4zCtTt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "-sbth09OCu1r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee613268-0025-48fe-e632-3fe378e848f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.2146 - accuracy: 0.9335\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 0.0728 - accuracy: 0.9781\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.0561 - accuracy: 0.9831\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0425 - accuracy: 0.9866\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 0.0366 - accuracy: 0.9882\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0515 - accuracy: 0.9864\n",
            "Test accuracy: 0.9864000082015991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adamax"
      ],
      "metadata": {
        "id": "k8jEhXAiCvNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Adamax',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "-XdfzY76Cv4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "604ef54d-6cf5-48a3-9107-9be008d8bb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.3265 - accuracy: 0.8997\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.1105 - accuracy: 0.9673\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0799 - accuracy: 0.9769\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 0.0649 - accuracy: 0.9806\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0556 - accuracy: 0.9830\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0508 - accuracy: 0.9851\n",
            "Test accuracy: 0.9850999712944031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nadam"
      ],
      "metadata": {
        "id": "l_i_bL7BCwkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "sXSC2h9zCxff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "499cd2c5-a355-45af-8ef0-f6a8e8e0f3e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 63s 32ms/step - loss: 0.2057 - accuracy: 0.9361\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 63s 34ms/step - loss: 0.0733 - accuracy: 0.9782\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0536 - accuracy: 0.9837\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0443 - accuracy: 0.9859\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0363 - accuracy: 0.9888\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0431 - accuracy: 0.9884\n",
            "Test accuracy: 0.9883999824523926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaGrad"
      ],
      "metadata": {
        "id": "1UVTQs_nCzns"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='AdaGrad',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "o3niZAzdCz5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d7101e7-7ac6-4e91-aa72-a2f62d214d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 1.9409 - accuracy: 0.3734\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.8666 - accuracy: 0.7395\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.5329 - accuracy: 0.8439\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.4126 - accuracy: 0.8795\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 67s 36ms/step - loss: 0.3515 - accuracy: 0.8955\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.2362 - accuracy: 0.9277\n",
            "Test accuracy: 0.9276999831199646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaDelta"
      ],
      "metadata": {
        "id": "TdsYUIFQC0nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='AdaDelta',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "id": "Hy0txb6AC1QC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8794f96c-f00f-41e0-9576-1a49438ce874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 65s 34ms/step - loss: 2.2959 - accuracy: 0.1285\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 2.2726 - accuracy: 0.1836\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 2.2469 - accuracy: 0.2405\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 2.2161 - accuracy: 0.2958\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 2.1747 - accuracy: 0.3421\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.1368 - accuracy: 0.5771\n",
            "Test accuracy: 0.5770999789237976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conclusion\n",
        "Best optimizer: Nadam, Test accuracy: 0.9883999824523926"
      ],
      "metadata": {
        "id": "V5LM8-T6C1jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experimenting Best Output Activation Function\n"
      ],
      "metadata": {
        "id": "bzThVcLDDrwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output relu"
      ],
      "metadata": {
        "id": "LL7SKTzYDvPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='relu')                                                       # Output layer with 10 neurons for classification (relu activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVNE1I0IO_Xn",
        "outputId": "3c1cbb0a-93fb-41a6-df12-6b034cb4b4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 79s 41ms/step - loss: 2.4114 - accuracy: 0.0986\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 2.3041 - accuracy: 0.0987\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 2.3033 - accuracy: 0.0987\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 63s 33ms/step - loss: 2.3037 - accuracy: 0.0987\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 2.3031 - accuracy: 0.0987\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3026 - accuracy: 0.0980\n",
            "Test accuracy: 0.09799999743700027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output LeakyReLU"
      ],
      "metadata": {
        "id": "lD6ioATRPRfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='LeakyReLU')                                                       # Output layer with 10 neurons for classification (LeakyReLU activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySPjILc1PTM0",
        "outputId": "aa9258a3-04ce-44b7-b6d3-dc22f8be3a60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 62s 32ms/step - loss: 2.5679 - accuracy: 0.3167\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 63s 34ms/step - loss: 2.3055 - accuracy: 0.1448\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 2.3030 - accuracy: 0.1442\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 65s 35ms/step - loss: 2.3029 - accuracy: 0.1328\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 2.3027 - accuracy: 0.1326\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3026 - accuracy: 0.1488\n",
            "Test accuracy: 0.14880000054836273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output elu"
      ],
      "metadata": {
        "id": "bOWN1YxyPT7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='elu')                                                       # Output layer with 10 neurons for classification (elu activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZp0U2FnPVAR",
        "outputId": "2e8f37fc-01c8-49e2-b455-6a61082a5a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 66s 34ms/step - loss: 2.3394 - accuracy: 0.2724\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 2.3043 - accuracy: 0.1256\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 2.3030 - accuracy: 0.1282\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 2.3030 - accuracy: 0.1203\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 63s 34ms/step - loss: 2.3027 - accuracy: 0.1160\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3026 - accuracy: 0.1157\n",
            "Test accuracy: 0.11569999903440475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output PReLU"
      ],
      "metadata": {
        "id": "zmH51vB4rs4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='PReLU')                                                       # Output layer with 10 neurons for classification (PReLU activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwRfWWMKruxm",
        "outputId": "5c0e7a2d-5ab0-4257-946f-fc631ad40a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 64s 33ms/step - loss: 2.2562 - accuracy: 0.3835\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 2.3067 - accuracy: 0.0996\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 2.3030 - accuracy: 0.0991\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 2.3034 - accuracy: 0.0989\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 2.3029 - accuracy: 0.0987\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3026 - accuracy: 0.0980\n",
            "Test accuracy: 0.09799999743700027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output sigmoid"
      ],
      "metadata": {
        "id": "-n60-nKmPVSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='sigmoid')                                                       # Output layer with 10 neurons for classification (sigmoid activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eLGdaOHPW2p",
        "outputId": "8a1f5d9e-d633-4082-e92a-3284744ece40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 61s 31ms/step - loss: 0.2073 - accuracy: 0.9360\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0729 - accuracy: 0.9783\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0566 - accuracy: 0.9830\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0439 - accuracy: 0.9868\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.0376 - accuracy: 0.9888\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0463 - accuracy: 0.9870\n",
            "Test accuracy: 0.9869999885559082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output tanh"
      ],
      "metadata": {
        "id": "aVFGI626PX2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='tanh')                                                       # Output layer with 10 neurons for classification (tanh activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vtQMLGVPY8_",
        "outputId": "edfc8d82-ce1d-4e8a-8413-da7334ba76f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 65s 33ms/step - loss: 2.2530 - accuracy: 0.2500\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 2.0639 - accuracy: 0.2898\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 2.4021 - accuracy: 0.2904\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 2.3094 - accuracy: 0.1020\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 2.3043 - accuracy: 0.0992\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3026 - accuracy: 0.0980\n",
            "Test accuracy: 0.09799999743700027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output softmax"
      ],
      "metadata": {
        "id": "k_ToRdIKPZyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the deep CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                                               # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Conv2D(128, (3, 3), activation='LeakyReLU'),                                             # Convolutional layer with 128 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                                                         # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                                                                # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                                           # Fully connected layer with 128 neurons and LeakyReLU activation\n",
        "    Dropout(0.5),                                                                                        # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                                       # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='Nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNDHNRPhPajZ",
        "outputId": "ecdec728-ff59-4362-ba8e-677263cd306d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 62s 32ms/step - loss: 0.2051 - accuracy: 0.9369\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 0.0737 - accuracy: 0.9771\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0573 - accuracy: 0.9832\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0459 - accuracy: 0.9863\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 66s 35ms/step - loss: 0.0375 - accuracy: 0.9879\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0483 - accuracy: 0.9852\n",
            "Test accuracy: 0.9851999878883362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conclusion\n",
        "Best output activation function: sigmoid, Test accuracy: 0.9869999885559082"
      ],
      "metadata": {
        "id": "nM5zToYQPbM1"
      }
    }
  ]
}