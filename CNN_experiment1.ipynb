{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHpo2NEK3DF9PfZTYaAO4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LokeRuiKee/ml-dl-playground/blob/main/CNN_experiment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Template Code"
      ],
      "metadata": {
        "id": "t_EJJSZvBM_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "3RcAQQPGBOAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfaaiRMxBOqr",
        "outputId": "113ed011-f999-4603-e6a4-aa0a76432939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 58s 30ms/step - loss: 0.2013 - accuracy: 0.9384\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0778 - accuracy: 0.9767\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0586 - accuracy: 0.9826\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0453 - accuracy: 0.9860\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0398 - accuracy: 0.9879\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 0.0255 - accuracy: 0.9914\n",
            "Test accuracy: 0.9914000034332275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 activation, optimizer = adam"
      ],
      "metadata": {
        "id": "v90B3v0N7vJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## relu"
      ],
      "metadata": {
        "id": "-asCL4U58MQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltBQrw78-VqU",
        "outputId": "4c4daf59-5296-4be0-a48d-2e000603deb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 58s 30ms/step - loss: 0.2063 - accuracy: 0.9369\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0790 - accuracy: 0.9766\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 57s 31ms/step - loss: 0.0576 - accuracy: 0.9825\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0466 - accuracy: 0.9861\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0390 - accuracy: 0.9882\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0236 - accuracy: 0.9928\n",
            "Test accuracy: 0.9927999973297119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LeakyReLU"
      ],
      "metadata": {
        "id": "fwexhTnX8OQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='LeakyReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='LeakyReLU'),                          # Convolutional layer with 64 filters of size 3x3 and LeakyReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='LeakyReLU'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qas-9-kJ8Nqz",
        "outputId": "e650a1cd-8773-4441-d692-14eb803c53c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.1544 - accuracy: 0.9530\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 63s 33ms/step - loss: 0.0628 - accuracy: 0.9808\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.0513 - accuracy: 0.9843\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 0.0402 - accuracy: 0.9870\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 0.0371 - accuracy: 0.9884\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0325 - accuracy: 0.9894\n",
            "Test accuracy: 0.9894000291824341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## elu"
      ],
      "metadata": {
        "id": "vtRtVF8o8TYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='elu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and elu activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='elu'),                          # Convolutional layer with 64 filters of size 3x3 and elu activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='elu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnIXqZaF8XJI",
        "outputId": "909a5763-d50d-489d-898c-9ee2df292525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 68s 36ms/step - loss: 0.1607 - accuracy: 0.9517\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.0736 - accuracy: 0.9777\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0594 - accuracy: 0.9815\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 0.0508 - accuracy: 0.9850\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0418 - accuracy: 0.9870\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0324 - accuracy: 0.9910\n",
            "Test accuracy: 0.9909999966621399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PReLU"
      ],
      "metadata": {
        "id": "cY46q_jT8Xz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='PReLU', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and PReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='PReLU'),                          # Convolutional layer with 64 filters of size 3x3 and PReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='PReLU'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DN2Uvd58Za_",
        "outputId": "a487f177-bd8d-45dd-9d79-3e5e18a443d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 124s 36ms/step - loss: 0.1842 - accuracy: 0.9438\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 0.0630 - accuracy: 0.9806\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 78s 41ms/step - loss: 0.0475 - accuracy: 0.9853\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 81s 43ms/step - loss: 0.0382 - accuracy: 0.9883\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 69s 37ms/step - loss: 0.0319 - accuracy: 0.9903\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0298 - accuracy: 0.9904\n",
            "Test accuracy: 0.9904000163078308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sigmoid"
      ],
      "metadata": {
        "id": "NklmCide8Zxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='sigmoid', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and sigmoid activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='sigmoid'),                          # Convolutional layer with 64 filters of size 3x3 and sigmoid activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='sigmoid'),                                   # Fully connected layer with 128 neurons and sigmoid activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s73fA-58bCT",
        "outputId": "add7eb34-88b0-4a79-b00c-d11b6cac38b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 1.2158 - accuracy: 0.5532\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.1618 - accuracy: 0.9529\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.1130 - accuracy: 0.9664\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0909 - accuracy: 0.9726\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0774 - accuracy: 0.9765\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0525 - accuracy: 0.9819\n",
            "Test accuracy: 0.9818999767303467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tanh"
      ],
      "metadata": {
        "id": "sDSARJSn8bUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='tanh', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and tanh activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='tanh'),                          # Convolutional layer with 64 filters of size 3x3 and tanh activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='tanh'),                                   # Fully connected layer with 128 neurons and tanh activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDKt3hIY8cg1",
        "outputId": "50493383-9c44-47c0-e842-227ac0b6ba04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.1661 - accuracy: 0.9503\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0747 - accuracy: 0.9776\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 0.0622 - accuracy: 0.9813\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0552 - accuracy: 0.9833\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 30ms/step - loss: 0.0474 - accuracy: 0.9849\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0397 - accuracy: 0.9862\n",
            "Test accuracy: 0.9861999750137329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## softmax"
      ],
      "metadata": {
        "id": "Kbhu7ZpD8eGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='softmax', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and softmax activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='softmax'),                          # Convolutional layer with 64 filters of size 3x3 and softmax activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='softmax'),                                   # Fully connected layer with 128 neurons and softmax activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm1XQFCg-7oP",
        "outputId": "dfa92d09-15ab-481d-b04b-5c1257934a2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 73s 39ms/step - loss: 1.9056 - accuracy: 0.2966\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 71s 38ms/step - loss: 1.4360 - accuracy: 0.4396\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 63s 34ms/step - loss: 1.2461 - accuracy: 0.5382\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 62s 33ms/step - loss: 1.1331 - accuracy: 0.5700\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 1.0898 - accuracy: 0.5874\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.3882 - accuracy: 0.9667\n",
            "Test accuracy: 0.96670001745224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conclusion\n",
        "Best activation function: relu. Test accuracy: 0.9927999973297119"
      ],
      "metadata": {
        "id": "unt3OKHW9BJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best activation function: relu, 8 optimizer"
      ],
      "metadata": {
        "id": "QcF55OTN9GCr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD"
      ],
      "metadata": {
        "id": "WmMSUuXr9IbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='SGD',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDBYGyQf9HBM",
        "outputId": "fa229783-17bb-454c-c042-f14b970a9696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.6601 - accuracy: 0.7931\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.2274 - accuracy: 0.9311\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.1675 - accuracy: 0.9498\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.1383 - accuracy: 0.9582\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.1214 - accuracy: 0.9638\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0621 - accuracy: 0.9791\n",
            "Test accuracy: 0.9790999889373779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SGD with Momentum"
      ],
      "metadata": {
        "id": "Cyd-aZEW9KSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "SGDm = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(optimizer= SGDm,\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d4KLHg99L47",
        "outputId": "e2bc4100-10d9-4288-8c6c-fe7faddfd23b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2684 - accuracy: 0.9147\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0883 - accuracy: 0.9730\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0681 - accuracy: 0.9797\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0552 - accuracy: 0.9834\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 50s 26ms/step - loss: 0.0469 - accuracy: 0.9858\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0291 - accuracy: 0.9897\n",
            "Test accuracy: 0.9897000193595886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSProp"
      ],
      "metadata": {
        "id": "MjJBsbmQ9Mbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='RMSProp',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17bZua4C9Nus",
        "outputId": "3713e953-75bd-48fd-fa2e-4eff4ca295ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 0.1838 - accuracy: 0.9439\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0680 - accuracy: 0.9803\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0529 - accuracy: 0.9843\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0476 - accuracy: 0.9860\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.0430 - accuracy: 0.9872\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0342 - accuracy: 0.9895\n",
            "Test accuracy: 0.9894999861717224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adam"
      ],
      "metadata": {
        "id": "oMiZDmDd9PKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xJCGCGS9PdW",
        "outputId": "787d005a-fd9f-46f0-9002-b6eed23ee201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 0.2087 - accuracy: 0.9363\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0765 - accuracy: 0.9776\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0557 - accuracy: 0.9833\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0477 - accuracy: 0.9858\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0388 - accuracy: 0.9881\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0228 - accuracy: 0.9925\n",
            "Test accuracy: 0.9925000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## adamax"
      ],
      "metadata": {
        "id": "XeAghzLc9PzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adamax',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBcUyWdR9Qs9",
        "outputId": "1c2f71e2-82cf-4445-824e-884359437a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 0.3472 - accuracy: 0.8949\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.1291 - accuracy: 0.9615\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0984 - accuracy: 0.9713\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0799 - accuracy: 0.9764\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0662 - accuracy: 0.9806\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0327 - accuracy: 0.9885\n",
            "Test accuracy: 0.9884999990463257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## nadam"
      ],
      "metadata": {
        "id": "o6plcMeX9Q_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='nadam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeEkKMFA9V1t",
        "outputId": "868ae03c-5a0e-4fe3-8629-cc0b22398b89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 54s 28ms/step - loss: 0.1943 - accuracy: 0.9421\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0737 - accuracy: 0.9782\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0533 - accuracy: 0.9846\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 0.0449 - accuracy: 0.9865\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.0359 - accuracy: 0.9893\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0265 - accuracy: 0.9904\n",
            "Test accuracy: 0.9904000163078308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaGrad"
      ],
      "metadata": {
        "id": "gFAmkd9_9V_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='AdaGrad',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUsz9WD29Wbq",
        "outputId": "d9faa965-80fe-404b-b1be-c111e4ac19e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 1.5277 - accuracy: 0.5276\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.6179 - accuracy: 0.8183\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.4564 - accuracy: 0.8647\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.3802 - accuracy: 0.8874\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 0.3408 - accuracy: 0.8999\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.2078 - accuracy: 0.9389\n",
            "Test accuracy: 0.9388999938964844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaDelta"
      ],
      "metadata": {
        "id": "JYHwzn7-9XZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='AdaDelta',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ty-uGAZh9Y0-",
        "outputId": "5d2ebf39-84a4-4036-e7b9-d592e5c9af9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 2.2759 - accuracy: 0.1522\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 2.2155 - accuracy: 0.2628\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 2.1475 - accuracy: 0.3632\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 2.0584 - accuracy: 0.4415\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 1.9487 - accuracy: 0.4925\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 1.8504 - accuracy: 0.7042\n",
            "Test accuracy: 0.704200029373169\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conclusion\n",
        "Best optimizer: Adam. Test accuracy: 0.9925000071525574"
      ],
      "metadata": {
        "id": "UnjowjtL9ZPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Experimenting Best Output Activation Function\n"
      ],
      "metadata": {
        "id": "1KAbXw0j9eVt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output relu"
      ],
      "metadata": {
        "id": "BXmnOLhO90AI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='relu')                                  # Output layer with 10 neurons for classification (relu activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq4BWS7d91fm",
        "outputId": "1eec345a-605a-4ecc-cd06-fe0f5d07e3cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 64s 34ms/step - loss: 2.4469 - accuracy: 0.1229\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 2.3091 - accuracy: 0.0987\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 2.3049 - accuracy: 0.0987\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3037 - accuracy: 0.0987\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 30ms/step - loss: 2.3037 - accuracy: 0.0987\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.3026 - accuracy: 0.0980\n",
            "Test accuracy: 0.09799999743700027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output LeakyReLU"
      ],
      "metadata": {
        "id": "6gmf7dRy94Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='LeakyReLU')                                  # Output layer with 10 neurons for classification (LeakyReLU activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v05S8H8kacrR",
        "outputId": "20859509-6b7f-4f81-957b-d8c9ec246cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 62s 32ms/step - loss: 2.4100 - accuracy: 0.1373\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 2.3062 - accuracy: 0.1087\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 2.3037 - accuracy: 0.1090\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3035 - accuracy: 0.1086\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 2.3026 - accuracy: 0.1089\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 2.3026 - accuracy: 0.1009\n",
            "Test accuracy: 0.10090000182390213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output elu"
      ],
      "metadata": {
        "id": "faEXvYlP96k8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='elu')                                  # Output layer with 10 neurons for classification (elu activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyDmKANc97QQ",
        "outputId": "168c5909-9fd3-4cf7-e6b0-b2888f6381fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 52s 27ms/step - loss: 2.4055 - accuracy: 0.1079\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3027 - accuracy: 0.1037\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 2.3026 - accuracy: 0.1046\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3026 - accuracy: 0.1060\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 52s 28ms/step - loss: 2.3026 - accuracy: 0.1040\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.3026 - accuracy: 0.1032\n",
            "Test accuracy: 0.10320000350475311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output PReLU"
      ],
      "metadata": {
        "id": "LCcnwoNy97kZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='PReLU')                                  # Output layer with 10 neurons for classification (PReLU activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-Fn0tkx98zP",
        "outputId": "b8b8ef81-fe73-4f44-fcdf-9c03b562baae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 54s 28ms/step - loss: 2.3867 - accuracy: 0.0991\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3032 - accuracy: 0.0987\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3032 - accuracy: 0.0987\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 50s 27ms/step - loss: 2.3040 - accuracy: 0.0987\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 2.3027 - accuracy: 0.0987\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 2.3026 - accuracy: 0.0980\n",
            "Test accuracy: 0.09799999743700027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output sigmoid"
      ],
      "metadata": {
        "id": "P1DSBgoA99RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='sigmoid')                                  # Output layer with 10 neurons for classification (sigmoid activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olS9Q6oQ9-Bw",
        "outputId": "1f1dac54-0191-4d70-a688-cab80c45d679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 53s 28ms/step - loss: 0.2012 - accuracy: 0.9390\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0734 - accuracy: 0.9773\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 51s 27ms/step - loss: 0.0569 - accuracy: 0.9833\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 53s 29ms/step - loss: 0.0446 - accuracy: 0.9865\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 54s 29ms/step - loss: 0.0379 - accuracy: 0.9886\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0256 - accuracy: 0.9927\n",
            "Test accuracy: 0.9926999807357788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output tanh"
      ],
      "metadata": {
        "id": "FqS92nMd9-aZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='tanh')                                  # Output layer with 10 neurons for classification (tanh activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZUtnBMe9_DO",
        "outputId": "0fcafb16-7550-4edf-900a-216ee1d8f67c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 2.4749 - accuracy: 0.1280\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 2.3082 - accuracy: 0.1228\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 32ms/step - loss: 2.3049 - accuracy: 0.1177\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 2.3041 - accuracy: 0.1098\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 57s 31ms/step - loss: 2.3041 - accuracy: 0.1024\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 2.3026 - accuracy: 0.0891\n",
            "Test accuracy: 0.08910000324249268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output softmax"
      ],
      "metadata": {
        "id": "GTr99wb39_eZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "# Define the architecture of the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),  # Convolutional layer with 32 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Conv2D(64, (3, 3), activation='relu'),                          # Convolutional layer with 64 filters of size 3x3 and ReLU activation\n",
        "    MaxPooling2D((2, 2)),                                           # Max pooling layer with pool size 2x2\n",
        "    Flatten(),                                                       # Flatten layer to convert 2D feature maps to 1D\n",
        "    Dense(128, activation='relu'),                                   # Fully connected layer with 128 neurons and ReLU activation\n",
        "    Dropout(0.5),                                                    # Dropout layer with dropout rate 0.5 for regularization\n",
        "    Dense(10, activation='softmax')                                  # Output layer with 10 neurons for classification (softmax activation)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',   # Using cross-entropy loss for classification\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Load and preprocess the data (example: MNIST dataset)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# Reshape and normalize the data\n",
        "x_train = x_train.reshape(-1, 28, 28, 1) / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1) / 255.0\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "On7ZidsC-ASs",
        "outputId": "40c0ade2-c8f2-4437-ea5b-6a4d4eada71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 60s 32ms/step - loss: 0.2028 - accuracy: 0.9380\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0748 - accuracy: 0.9779\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 61s 33ms/step - loss: 0.0534 - accuracy: 0.9845\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 57s 31ms/step - loss: 0.0428 - accuracy: 0.9874\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 58s 31ms/step - loss: 0.0349 - accuracy: 0.9893\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0268 - accuracy: 0.9925\n",
            "Test accuracy: 0.9925000071525574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## conclusion\n",
        "Best output activation function: sigmoid. Test accuracy: 0.9926999807357788"
      ],
      "metadata": {
        "id": "ABhk2FN5-AlJ"
      }
    }
  ]
}